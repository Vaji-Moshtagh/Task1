{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FIGRURES_DIR, RESULT_DIR\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrobobo_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     IRobobo,\n\u001b[1;32m      5\u001b[0m     Emotion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     HardwareRobobo,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from data_files import FIGRURES_DIR, RESULT_DIR\n",
    "from robobo_interface import (\n",
    "    IRobobo,\n",
    "    Emotion,\n",
    "    LedId,\n",
    "    LedColor,\n",
    "    SoundEmotion,\n",
    "    SimulationRobobo,\n",
    "    HardwareRobobo,\n",
    ")\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "class GymEnv(gym.Env):\n",
    "\n",
    "    # We don't need metadata, but the superclass wants to see it\n",
    "    metadata = {\"render_modes\": [\"dummy\"]}\n",
    "\n",
    "    def __init__(self, render_mode=None, rob=IRobobo, max_steps=100):\n",
    "        super(GymEnv, self).__init__()\n",
    "\n",
    "\n",
    "        assert render_mode is None\n",
    "        assert rob != False\n",
    "        self.rob = rob\n",
    "\n",
    "        # Maximum number of steps per episode\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Define action space\n",
    "        self.action_space = gym.spaces.Box(low=np.array([0]), high=np.array([5]), dtype=np.float32)\n",
    "\n",
    "        # Define observation space: assume 8 continuous observations ranging from 0 to 10000\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=200, shape=(8,), dtype=np.float32)\n",
    "\n",
    "        # Initialise for logging\n",
    "        self.log_irsdata = []\n",
    "        self.log_rewards = []\n",
    "        self.log_collision = []\n",
    "        self.log_actions = []\n",
    "        self.total_steps = 0\n",
    "        self.total_timesteps_in_learn = 500\n",
    "\n",
    "    def _move(self, action):\n",
    "        # self.rob.move_blocking(action[0], action[1], 100)\n",
    "\n",
    "        action = action[0]\n",
    "\n",
    "        # if action < 1:\n",
    "        #     self.rob.move_blocking(0, 100, 100)\n",
    "        # elif action < 2:\n",
    "        #     self.rob.move_blocking(50, 100, 100)\n",
    "        # elif action < 3:\n",
    "        #     self.rob.move_blocking(100, 100, 100)\n",
    "        # elif action < 4:\n",
    "        #     self.rob.move_blocking(100, 50, 100)\n",
    "        # else:\n",
    "        #     self.rob.move_blocking(100, 0, 100)\n",
    "\n",
    "        left = max(2.5 - action, 0) * 100 / 2.5\n",
    "        right = max(action - 2.5, 0) * 100 / 2.5\n",
    "        self.rob.move_blocking(left, right, 100)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.rob.read_irs()\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {'dummy_info': 0}\n",
    "\n",
    "    def _spin_at_episode_start(self):\n",
    "        random_amount = np.random.randint(0, 1001)\n",
    "        self.rob.move_blocking(100, -100, random_amount)\n",
    "\n",
    "    def _calculate_max_distance(self):\n",
    "        if len(self.visited_positions) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate the pairwise differences\n",
    "        diffs = self.visited_positions[:, np.newaxis, :] - self.visited_positions[np.newaxis, :, :]\n",
    "\n",
    "        # Compute the Euclidean distances\n",
    "        distances = np.sqrt(np.sum(diffs**2, axis=-1))\n",
    "\n",
    "        # Find the maximum distance\n",
    "        max_distance = np.max(distances)\n",
    "\n",
    "        return max_distance\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # This line is probably needed but does nothing\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        if isinstance(self.rob, SimulationRobobo):\n",
    "            if self.rob.is_running():\n",
    "                self.rob.stop_simulation()\n",
    "\n",
    "            self.rob.play_simulation()\n",
    "        else:\n",
    "            self.rob.talk(\"Put me back\")\n",
    "            self.rob.sleep(5)\n",
    "\n",
    "        # For maximizing explored distance reward method\n",
    "        self.visited_positions = np.array([[self.rob.get_position().x, self.rob.get_position().y]])\n",
    "        self.best_distance = 0\n",
    "\n",
    "        # self._spin_at_episode_start()\n",
    "\n",
    "        # Initialize step counter\n",
    "        self.step_count = 0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Take the action\n",
    "        self._move(action)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # Save the new position for keeping track of how much distance the robot covered\n",
    "        position = np.array([[self.rob.get_position().x, self.rob.get_position().y]])\n",
    "        self.visited_positions = np.vstack((self.visited_positions, position))\n",
    "\n",
    "        # # Reward based on maximizing explored distance method\n",
    "        # new_best_distance = self._calculate_max_distance()\n",
    "        # reward = new_best_distance - self.best_distance # if improved: value, else: 0\n",
    "        # reward *= 10\n",
    "        # self.best_distance = new_best_distance\n",
    "\n",
    "        # Reward based on wheels difference\n",
    "        reward = - abs(action[0] - 2.5)\n",
    "\n",
    "        # # Reward update for collision punishment\n",
    "        # if max(observation) > 100:\n",
    "        #     reward -= 0.5\n",
    "\n",
    "        # Increment step\n",
    "        self.step_count += 1\n",
    "        self.total_steps += 1 # Only for logging\n",
    "\n",
    "        # Determine if the episode is terminated based on the number of steps\n",
    "        terminated = self.step_count >= self.max_steps\n",
    "\n",
    "        # TODO early termination if max distance has been found\n",
    "\n",
    "        # Logging\n",
    "        self.log_irsdata.append(observation)\n",
    "        self.log_rewards.append(reward)\n",
    "        self.log_collision.append(max(observation) > 100)\n",
    "        self.log_actions.append(action)\n",
    "        if self.total_steps == self.total_timesteps_in_learn:\n",
    "            with open(str(RESULT_DIR / 'data/run_2/picklec'), 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'irsdata': self.log_irsdata,\n",
    "                    'rewards': self.log_rewards,\n",
    "                    'collision': self.log_collision,\n",
    "                    'actions': self.log_actions\n",
    "                }, f)\n",
    "        # More Logging\n",
    "            with open(str(RESULT_DIR / 'data/run_2/irsc.csv'), 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['observation'])\n",
    "                writer.writerows([[obs] for obs in self.log_irsdata])\n",
    "            with open(str(RESULT_DIR / 'data/run_2/rewards1.csv'), 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['reward'])\n",
    "                writer.writerows([[reward] for reward in self.log_rewards])\n",
    "            with open(str(RESULT_DIR / 'data/run_2/collisionc.csv'), 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['collision'])\n",
    "                writer.writerows([[collision] for collision in self.log_collision])\n",
    "            with open(str(RESULT_DIR / 'data/run_2/actions1.csv'), 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['actions'])\n",
    "                writer.writerows([[action] for action in self.log_actions])\n",
    "        # /Logging\n",
    "\n",
    "        # Output index 3 has to be False, because it is a deprecated feature\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "\n",
    "def test_emotions(rob: IRobobo):\n",
    "    rob.set_emotion(Emotion.HAPPY)\n",
    "    rob.talk(\"Hello\")\n",
    "    rob.play_emotion_sound(SoundEmotion.PURR)\n",
    "    rob.set_led(LedId.FRONTCENTER, LedColor.GREEN)\n",
    "\n",
    "\n",
    "def test_move_and_wheel_reset(rob: IRobobo):\n",
    "    rob.move_blocking(100, 100, 1000)\n",
    "    print(\"before reset: \", rob.read_wheels())\n",
    "    rob.reset_wheels()\n",
    "    rob.sleep(1)\n",
    "    print(\"after reset: \", rob.read_wheels())\n",
    "\n",
    "\n",
    "def test_sensors(rob: IRobobo):\n",
    "    print(\"IRS data: \", rob.read_irs())\n",
    "    image = rob.get_image_front()\n",
    "    cv2.imwrite(str(FIGRURES_DIR / \"photo.png\"), image)\n",
    "    print(\"Phone pan: \", rob.read_phone_pan())\n",
    "    print(\"Phone tilt: \", rob.read_phone_tilt())\n",
    "    print(\"Current acceleration: \", rob.read_accel())\n",
    "    print(\"Current orientation: \", rob.read_orientation())\n",
    "\n",
    "\n",
    "def test_phone_movement(rob: IRobobo):\n",
    "    rob.set_phone_pan_blocking(20, 100)\n",
    "    print(\"Phone pan after move to 20: \", rob.read_phone_pan())\n",
    "    rob.set_phone_tilt_blocking(50, 100)\n",
    "    print(\"Phone tilt after move to 50: \", rob.read_phone_tilt())\n",
    "\n",
    "\n",
    "def test_sim(rob: SimulationRobobo):\n",
    "    print(rob.get_sim_time())\n",
    "    print(rob.is_running())\n",
    "    rob.stop_simulation()\n",
    "    print(rob.get_sim_time())\n",
    "    print(rob.is_running())\n",
    "    rob.play_simulation()\n",
    "    print(rob.get_sim_time())\n",
    "    print(rob.get_position())\n",
    "\n",
    "\n",
    "def test_hardware(rob: HardwareRobobo):\n",
    "    print(\"Phone battery level: \", rob.read_phone_battery())\n",
    "    print(\"Robot battery level: \", rob.read_robot_battery())\n",
    "\n",
    "\n",
    "def run_all_actions(rob: IRobobo):\n",
    "    if isinstance(rob, SimulationRobobo):\n",
    "        rob.play_simulation()\n",
    "    test_emotions(rob)\n",
    "    test_sensors(rob)\n",
    "    test_move_and_wheel_reset(rob)\n",
    "    if isinstance(rob, SimulationRobobo):\n",
    "        test_sim(rob)\n",
    "\n",
    "    if isinstance(rob, HardwareRobobo):\n",
    "        test_hardware(rob)\n",
    "\n",
    "    test_phone_movement(rob)\n",
    "\n",
    "    if isinstance(rob, SimulationRobobo):\n",
    "        rob.stop_simulation()\n",
    "\n",
    "\n",
    "def test(rob: IRobobo):\n",
    "    # Start the simulation\n",
    "    if isinstance(rob, SimulationRobobo):\n",
    "        rob.play_simulation()\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        rob.move_blocking(100, 0, 250)\n",
    "\n",
    "    # Stop simulation\n",
    "    if isinstance(rob, SimulationRobobo):\n",
    "        rob.stop_simulation()\n",
    "\n",
    "\n",
    "def task0(rob: IRobobo):\n",
    "    # Start the simulation\n",
    "    if isinstance(rob, SimulationRobobo):\n",
    "        rob.play_simulation()\n",
    "\n",
    "    # If sensor sees something: turn right, if not: straight ahead\n",
    "    for _ in range(100):\n",
    "\n",
    "        # Somehow the first value for read_irs() in the simulation is always [inf, inf, ...]\n",
    "        # so the first value is hard_set to 0\n",
    "        if _ == 0:\n",
    "            irs = [0,0,0]\n",
    "        else:\n",
    "            # Read FrontL, FrontR, FrontC infrared sensor\n",
    "            irs = rob.read_irs()[2:5]\n",
    "\n",
    "        print(irs)\n",
    "\n",
    "        # Turn right or move straight\n",
    "        if max(irs) > 100:\n",
    "            rob.move_blocking(50, -100, 250)\n",
    "        else:\n",
    "            rob.move_blocking(50, 50, 100)\n",
    "\n",
    "    # Stop simulation\n",
    "    if isinstance(rob, SimulationRobobo):\n",
    "        rob.stop_simulation()\n",
    "\n",
    "\n",
    "def task1(rob: IRobobo):\n",
    "\n",
    "    model = DDPG(\n",
    "        policy = 'MlpPolicy',\n",
    "        env = GymEnv(rob=rob, max_steps=100),\n",
    "        learning_rate=0.001,\n",
    "        buffer_size=50000,\n",
    "        learning_starts=100,\n",
    "        batch_size=256,\n",
    "        tau=0.005,\n",
    "        gamma=0.99,\n",
    "        train_freq=1,\n",
    "        gradient_steps=1,\n",
    "        action_noise=NormalActionNoise(mean=np.zeros(1), sigma=0.1 * np.ones(1)), # Change this too if the action space changes shape\n",
    "        replay_buffer_class=None,\n",
    "        replay_buffer_kwargs=None,\n",
    "        optimize_memory_usage=False,\n",
    "        tensorboard_log=None,\n",
    "        policy_kwargs=None,\n",
    "        verbose=0,\n",
    "        seed=None,\n",
    "        device='auto',\n",
    "        _init_setup_model=True\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=500, log_interval=10, progress_bar=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Rewards Over Time')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    model.save(str(RESULT_DIR / 'models/run_2c'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
